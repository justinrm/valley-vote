# Valley Vote: Legislative Vote Prediction

## Overview

Valley Vote is a project aimed at predicting how legislators in the Idaho State Legislature will vote on specific bills. By leveraging machine learning techniques and diverse data sources, the project seeks to understand the factors influencing legislative decision-making. The primary goal is to build a predictive model (currently planned using XGBoost) trained on historical voting patterns, legislator attributes, bill characteristics, and external influences like campaign finance and district demographics.

This project can provide insights for constituents, advocacy groups, researchers, and policymakers interested in legislative behavior.

## Features

**Current:**

*   **Comprehensive Data Collection:** Fetches extensive legislative data via the LegiScan API, including:
    *   Legislative Sessions
    *   Legislator Profiles (party, district, role)
    *   Committee Definitions & Structures
    *   Bill Details (text, status, subjects)
    *   Sponsorship Records
    *   Detailed Roll Call Vote Records
*   **Targeted Web Scraping:** 
    *   Retrieves current-year committee membership data directly from the Idaho Legislature website
    *   Implements campaign finance data collection from the Idaho Secretary of State's Sunshine Portal (in progress)
*   **Name Matching:** Uses fuzzy logic (`fuzzywuzzy`) to link legislator names from scraped sources to official LegiScan legislator IDs.
*   **Structured Data Output:** 
    *   Saves collected data in both raw JSON format (for auditability) and processed CSV format (for ease of use in analysis)
    *   Organizes data by type, year, and state
    *   Implements consistent error handling and logging across all data collection modules
*   **Robust Operations:** 
    *   Employs retry mechanisms (`tenacity`) for resilient API calls and web scraping
    *   Handles rate limiting, timeouts, and connection issues gracefully
    *   Implements exponential backoff for failed requests
*   **Enhanced Error Handling:**
    *   Comprehensive logging with detailed error messages and debugging information
    *   Graceful fallbacks for missing or incomplete data
    *   Data validation at multiple stages of collection
*   **Configurability:** 
    *   Allows specifying state, year range, and skipping specific collection steps via command-line arguments
    *   Supports custom data directory configuration
    *   Flexible retry and timeout settings

**Planned:**

*   **Expanded Data Integration:** Incorporate additional crucial datasets:
    *   **Campaign Finance:** 
        * Currently implementing contribution data collection from Idaho SOS Sunshine portal
        * Support for donor industry categorization
        * Automated matching of contributions to legislators
    *   **District Demographics:** Population, income, education levels per legislative district (from US Census/TIGER).
    *   **Election History:** Legislator election margins, district competitiveness (from Idaho SOS Election results).
*   **Advanced Feature Engineering:**
    *   Calculate legislator **influence scores** (based on sponsorships, committee roles, seniority).
    *   Develop features reflecting party loyalty, voting history patterns, and bill complexity.
*   **Predictive Modeling:**
    *   Implement an **XGBoost** model for vote prediction (Yea/Nay).
    *   Perform hyperparameter tuning and cross-validation.
*   **Model Interpretability:** Utilize **SHAP** or similar techniques to understand which factors most significantly influence predictions.
*   **Experiment Tracking:** (Optional) Use tools like MLflow to manage experiments and model versions.

## Project Status (As of current review)

*   **Phase 1: Data Acquisition:**
    *   **Complete:** Core LegiScan API data collection (`data_collection.py`).
    *   **Complete:** Idaho-specific committee membership scraping & matching (`data_collection.py`).
    *   **In Progress:** Campaign finance data collection (`scrape_finance_idaho.py`).
    *   **To Do:** Implementation of collectors/parsers for District Demographics and Election History.
*   **Phase 2: Preprocessing & Feature Engineering:** **Not Started.** Requires implementation (`data_preprocessing.py`).
*   **Phase 3: Modeling (XGBoost):** **Not Started.** Requires implementation (`xgboost_model.py`).
*   **Phase 4: Evaluation & Deployment:** **Not Started.**
*   **Phase 5: Documentation & Maintenance:** **In Progress.**

For a detailed breakdown of pending tasks, please see the `TODO.md` file.

## Data Sources

*   **LegiScan API:** Primary source for sessions, legislators, bills, votes, committees, sponsors. (Implemented)
*   **Idaho Legislature Website:** Source for current committee memberships via web scraping. (Implemented - ID only)
*   **Idaho Secretary of State (Sunshine Portal):** Source for campaign finance data. (Implementation in Progress)
*   **U.S. Census Bureau (ACS & TIGER/Line):** Planned source for district demographics. (Requires Implementation)
*   **Idaho Secretary of State (Election Results):** Planned source for historical election data. (Requires Implementation)

## High-Level Workflow

1.  **Collect Core Data (`data_collection.py`):** 
    * Fetch data from LegiScan API
    * Scrape committee memberships
    * Process and match legislator names
    * Output raw JSON and processed yearly CSVs
2.  **Collect Campaign Finance (`scrape_finance_idaho.py`):**
    * Scrape contribution data from Sunshine Portal
    * Match donors and recipients
    * Process and validate contribution records
3.  **(Planned) Collect Additional Data:** Run separate scripts for Demographics and Elections.
4.  **(Planned) Preprocess & Feature Engineer (`data_preprocessing.py`):** Load all collected data, clean it, perform crucial matching, engineer features, and create a final feature matrix for modeling.
5.  **(Planned) Train & Evaluate Model (`xgboost_model.py`):** Use the feature matrix to train the XGBoost model, tune hyperparameters, evaluate performance, and save the model.
6.  **(Planned) Predict & Interpret:** Use the trained model to predict votes on new bills and analyze feature importance (SHAP).

## Directory Structure

```
valley-vote/
├── data/
│   ├── raw/              # Raw data dumps (JSON primarily)
│   │   ├── legislators/
│   │   ├── committees/
│   │   ├── bills/
│   │   ├── votes/
│   │   ├── sponsors/
│   │   ├── committee_memberships/
│   │   ├── campaign_finance/ # (Planned)
│   │   ├── demographics/     # (Planned)
│   │   └── elections/        # (Planned)
│   └── processed/        # Processed, cleaned data (CSVs primarily)
│
├── docs/                 # Documentation files
│   ├── data_collection_readme.md
│   └── (Other docs like data_schema.md, feature_engineering.md - Planned)
│
├── data_collection.py    # Main script for API/Scraping data collection
├── data_preprocessing.py # (Planned) Script for cleaning, merging, feature engineering
├── xgboost_model.py      # (Planned) Script for XGBoost model training/evaluation
├── scrape_finance_idaho.py # (Planned Example) Dedicated finance scraper
├── process_demographics_idaho.py # (Planned Example) Dedicated demographics processor
├── parse_elections_idaho.py # (Planned Example) Dedicated election parser
├── requirements.txt      # Project dependencies
├── todo.md               # Detailed task list
└── README.md             # This file
```

## Setup & Installation

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/justinrm/valley-vote
    cd valley-vote
    ```
2.  **Set API Key:** Obtain an API key from [LegiScan](https://legiscan.com/user/register) and set it as an environment variable:
    *   Linux/macOS: `export LEGISCAN_API_KEY='your_actual_api_key'`
    *   Windows (cmd): `set LEGISCAN_API_KEY=your_actual_api_key`
    *   Windows (PowerShell): `$env:LEGISCAN_API_KEY='your_actual_api_key'`
    *   *Note: The `data_collection.py` script will exit if this variable is not set.*
3.  **Install Dependencies:** Create a virtual environment (recommended) and install the required packages.
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows use `venv\Scripts\activate`
    pip install -r requirements.txt
    ```
    *(Note: Ensure `requirements.txt` is created and lists packages like `requests`, `pandas`, `tenacity`, `tqdm`, `beautifulsoup4`, `fuzzywuzzy`, `python-Levenshtein`, and eventually `xgboost`, `geopandas`, `scikit-learn`, `shap`, `mlflow` etc.)*

## Usage

Currently, the main runnable script is `data_collection.py`.

```bash
# Example: Collect data for Idaho for years 2022-2023 (includes scraping/matching)
python data_collection.py --state ID --start-year 2022 --end-year 2023

# Example: Collect API data only for California 2023 (skips scraping/matching)
python data_collection.py --state CA --start-year 2023 --end-year 2023 --skip-scraping --skip-matching

# See the script's help for more options:
python data_collection.py --help
```

Other scripts (`data_preprocessing.py`, `xgboost_model.py`, etc.) are planned and not yet executable.

## Key Scripts

*   **`data_collection.py`**: Fetches data from LegiScan API and performs state-specific web scraping (currently Idaho committees). Outputs raw and processed data files. See `docs/readme_data_collection.md` for details.
*   **`data_preprocessing.py` (Planned)**: Will load data from `data_collection.py` and other sources, clean, merge, match records across sources, and engineer features for modeling.
*   **`xgboost_model.py` (Planned)**: Will implement the XGBoost model training, evaluation, prediction, and interpretation pipeline.
*   **(Dedicated Collection Scripts - Planned)**: Separate scripts like `scrape_finance_idaho.py`, `process_demographics_idaho.py`, `parse_elections_idaho.py` will handle the specific logic for acquiring new data sources.

## Future Work & Roadmap

The immediate next steps involve implementing the data collection for the planned sources (Campaign Finance, Demographics, Elections) and then developing the `data_preprocessing.py` script to integrate all data and create features. Following that, the `xgboost_model.py` script will be developed.

Please refer to `todo.md` for a detailed list of tasks and their status.

## License

MIT LICENSE
