
# Project TODO List: Valley Vote (Legislative Vote Prediction)

This document tracks the tasks required to build and improve the legislative vote prediction model.

## Phase 1: Enhanced Data Acquisition

*Core LegiScan API collection and basic scraping are complete via `data_collection.py`.*

-   [x] Implement core LegiScan API fetching (Sessions, Legislators, Committees, Bills, Votes, Sponsors) (`data_collection.py`)
-   [x] Implement basic committee membership web scraping (Idaho specific) (`data_collection.py`)
-   [x] Implement fuzzy name matching for scraped committee members (`data_collection.py`)
-   [x] Implement data structuring and saving (Raw JSONs, Processed CSVs) (`data_collection.py`)
-   [x] Implement basic consolidation of yearly API/scraped data (`data_collection.py`)
-   [x] **Implement Campaign Finance Scraper/Collector (`scrape_finance_idaho.py` or similar)**
    -   [x] Target Idaho Sunshine portal (or relevant state source).
    -   [x] Identify legislator/candidate filings.
    -   [x] Extract contributions (amount, date, donor, donor industry/type if possible).
    -   [~] Extract expenditures (optional).
    -   [x] Save raw scraped data/extractions.
    -   [x] Develop strategy for robustly matching finance records (candidate names) to LegiScan `legislator_id`.
-   [ ] **Implement Demographics Collector/Processor (`process_demographics_idaho.py` or similar)**
    -   [ ] Identify and download relevant Census ACS tables (income, education, age, race etc.).
    -   [ ] Identify and download TIGER/Line shapefiles for state legislative districts.
    -   [ ] Use `geopandas` (or similar GIS tools) to spatially join Census data to district boundaries.
    -   [ ] Aggregate demographic statistics per legislative district.
    -   [ ] Save processed demographics per district (e.g., CSV or GeoJSON).
-   [ ] **Implement Election History Parser (`parse_elections_idaho.py` or similar)**
    -   [ ] Identify and download historical election result files (e.g., from `voteidaho.gov`).
    -   [ ] Parse relevant file formats (Excel, CSV).
    -   [ ] Extract results for state legislative races (candidate, party, votes, total votes, district, year).
    -   [ ] Calculate margin of victory / vote share.
    -   [ ] Develop strategy for robustly matching election candidates to LegiScan `legislator_id`.
    -   [ ] Save raw parsed data and/or processed results.
-   [ ] (Optional) Refine state-specific web scrapers for robustness or add support for other states.
-   [ ] (Optional) Implement caching for API calls and web requests to improve speed and reduce load.

## Phase 2: Preprocessing & Feature Engineering

*This phase focuses on cleaning, merging, and creating features from the collected data.*

-   [ ] **Create `data_preprocessing.py` script:** This will be the central script for this phase.
-   [ ] **Implement Master Data Loading:** Load all relevant processed CSVs from Phase 1 (`legislators`, `votes`, `bills`, `committees`, `sponsors`, matched memberships, finance, demographics, elections).
-   [ ] **Implement Robust Data Linking:**
    -   [ ] Finalize matching logic for campaign finance records to `legislator_id`.
    -   [ ] Finalize matching logic for election history records to `legislator_id`.
    -   [ ] Ensure consistent `legislator_id`, `bill_id`, `vote_id`, `committee_id`, `district_id` across datasets.
-   [ ] **Implement Data Cleaning:**
    -   [ ] Handle missing values appropriately (imputation, removal).
    *   [ ] Standardize data formats (dates, text).
    *   [ ] Address inconsistencies found during EDA.
-   [ ] **Implement Feature Engineering:**
    -   [ ] **Calculate Legislative Influence Score:** Define and compute score based on factors like bills sponsored/passed, committee leadership roles (chair, vice-chair), seniority (requires tracking terms/sessions).
    -   [ ] Engineer features from **votes:** Lagged vote history, voting patterns.
    -   [ ] Engineer features from **bills:** Subject matter categories (e.g., one-hot encode `subjects`), bill complexity (e.g., length of text - needs fetching bill text).
    -   [ ] Engineer features from **committees:** Legislator's committee assignments (one-hot), committee influence/type.
    -   [ ] Engineer features from **demographics:** Link district demographics to legislators/votes.
    -   [ ] Engineer features from **campaign finance:** Donor industry influence, contribution amounts/patterns.
    -   [ ] Engineer features from **election history:** Margin of victory, district competitiveness.
    -   [ ] Engineer features related to **party:** Party alignment scores, voting with/against party majority.
-   [ ] **Generate Final Feature Matrix:** Create the final dataset (`voting_data.csv` or similar) with target variable (`vote_value` 0/1) and all engineered features, ready for modeling. Filter out non-binary votes (abstain, absent etc.) unless explicitly modeling them.

## Phase 3: Modeling (XGBoost Focus)

*This phase focuses on building and training the predictive model.*

-   [ ] **Create `xgboost_model.py` script** (or Jupyter Notebook for exploration).
-   [ ] Load the final feature matrix from Phase 2.
-   [ ] Implement data splitting strategy (e.g., time-based split for legislative sessions, standard train/validation/test).
-   [ ] Handle categorical features for XGBoost (e.g., One-Hot Encoding, potentially Target Encoding with care).
-   [ ] Implement XGBoost model training.
-   [ ] Perform hyperparameter tuning (e.g., using GridSearchCV, RandomizedSearchCV, or Optuna).
-   [ ] Save the trained model artifact (e.g., using `joblib` or XGBoost's internal save methods).
-   [ ] Implement feature importance analysis (using XGBoost's built-in importance and/or SHAP).
-   [ ] (Optional) Experiment with baseline models (e.g., Logistic Regression) for comparison.
-   [ ] (Optional) Experiment with other tree-based models (Random Forest, LightGBM) or ensembles.

## Phase 4: Evaluation & Deployment

*This phase focuses on assessing model performance and potentially deploying it.*

-   [ ] Implement robust model evaluation using appropriate metrics (Accuracy, Precision, Recall, F1-score, ROC-AUC).
-   [ ] Perform evaluation on the hold-out test set.
-   [ ] Analyze model predictions (e.g., confusion matrix, calibration plots).
-   [ ] Implement cross-validation during training/tuning for more reliable performance estimates.
-   [ ] (Optional) Set up experiment tracking (e.g., MLflow) to log parameters, metrics, and model artifacts.
-   [ ] (Optional) Develop a simple prediction interface (e.g., a function to predict votes on new bills, or a basic Flask API).
-   [ ] (Optional) Design and implement a strategy for model retraining and monitoring.

## Phase 5: Documentation & Maintenance

*Ongoing tasks to ensure project quality and usability.*

-   [x] Create initial `data_collection_readme.md`.
-   [x] Create initial project `TODO.md`.
-   [ ] **Update Documentation:** Keep READMEs for each script (`data_collection`, `preprocessing`, `modeling`) updated as features are added/changed.
-   [ ] **Update Main Project README:** Ensure the top-level README provides a good overview, links to other docs, and setup instructions.
-   [ ] **Document Data Schema:** Create `docs/data_schema.md` describing the fields in the key processed CSV files.
-   [ ] **Document Feature Engineering:** Create `docs/feature_engineering.md` explaining how key features (like Influence Score) are calculated.
-   [ ] **Add Code Comments:** Ensure code is well-commented.
-   [ ] **Add Unit/Integration Tests:** Implement tests for critical functions (API parsing, matching logic, feature calculations).
-   [ ] **Maintain `requirements.txt`:** Keep the list of dependencies up-to-date.
-   [ ] **Refactor Code:** Improve code quality, efficiency, and readability as needed.
-   [ ] Monitor data sources (API changes, website structure changes) and update collectors/parsers accordingly.
